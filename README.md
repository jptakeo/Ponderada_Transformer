# Ponderada â€” Translation with a Transformer (PTâ†’EN)

RepositÃ³rio da **ponderada de programaÃ§Ã£o** do MÃ³dulo 11 de CiÃªncia da ComputaÃ§Ã£o: **â€œAtividade: TraduÃ§Ã£o usando Transformer com Controle de VersÃµesâ€**.

Implementa um **Transformer encoderâ€“decoder** seguindo o tutorial oficial do TensorFlow para **traduÃ§Ã£o automÃ¡tica de portuguÃªs â†’ inglÃªs**, baseado no paper _Attention Is All You Need_. O projeto foi executado e documentado em notebook, com anÃ¡lise crÃ­tica de resultados e comparaÃ§Ã£o **CPU vs GPU**.



## VisÃ£o Geral do Projeto

- **Objetivo:** reproduzir e analisar um modelo Transformer para traduÃ§Ã£o PTâ†’EN.
- **Base:** tutorial _Neural machine translation with a Transformer and Keras_ (TensorFlow).
- **Escopo:** treino do zero em ~50k pares PTâ€“EN (TED Talks), avaliaÃ§Ã£o simples e discussÃ£o tÃ©cnica.
- **Entrega:** notebook `transformer.ipynb` + este `README.md`.


## Arquitetura Implementada

### Componentes
- **Encoder:** 4 camadas com Multiâ€‘Head Attention + Feedâ€‘Forward.
- **Decoder:** 4 camadas com selfâ€‘attention mascarada + crossâ€‘attention.
- **Embedding:** dimensÃ£o **128** com **codificaÃ§Ã£o posicional** sinusoidal.
- **TokenizaÃ§Ã£o:** **subpalavras** via **BertTokenizer** (otimizado para PT/EN).
- **AtenÃ§Ã£o:** **8 cabeÃ§as** (multiâ€‘head).

### HiperparÃ¢metros
```python
num_layers   = 4
d_model      = 128
dff          = 512
num_heads    = 8
dropout_rate = 0.1
batch_size   = 64
epochs       = 20
```

> Nota: parÃ¢metros foram **reduzidos** em relaÃ§Ã£o ao paper original (d_model=512, 6 camadas) para viabilizar a execuÃ§Ã£o em recursos limitados.



## Dataset

- **Fonte:** TED Talks PTâ€“EN
- **Treino:** ~50.000 pares de sentenÃ§as  
- **ValidaÃ§Ã£o:** ~1.100 exemplos  
- **Teste:** ~2.000 exemplos

O pipeline utiliza batching, prefetch e paralelizaÃ§Ã£o para melhor throughput. O tokenizador de subpalavras reduz OOV e melhora a generalizaÃ§Ã£o.



## Como Reproduzir

### 1) Requisitos
- Python 3.10+
- TensorFlow 2.x (GPU recomendado)
- CUDA/cuDNN compatÃ­veis (se usar GPU)
- Demais libs listadas em `requirements.txt`

```bash
# ambiente (exemplo)
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 2) Executar o notebook
Abra e rode **`transformer.ipynb`** (Jupyter/VSCode/Colab).  
O notebook contÃ©m: preparaÃ§Ã£o de dados, definiÃ§Ã£o do modelo, treino, inferÃªncia e visualizaÃ§Ãµes de atenÃ§Ã£o.

### 3) Exportar o modelo (opcional)
```python
# exemplo no notebook
tf.saved_model.save(model, "export/saved_model")
```



## Resultados de Treinamento (logs)

| Ã‰poca | Loss | AcurÃ¡cia |
|:----:|:----:|:--------:|
| 1 | 6.70 | 11.39% |
| 10 | 2.11 | 58.25% |
| 20 | 1.45 | 67.99% |

> MÃ©tricas adicionais como **BLEU/ROUGE/BERTScore** nÃ£o foram computadas nesta versÃ£o (ver _Roadmap_).



## Exemplos de TraduÃ§Ã£o (PT â†’ EN)

| PortuguÃªs | PrediÃ§Ã£o | Ground Truth |
|---|---|---|
| este Ã© um problema que temos que resolver. | this is a problem we have to solve . | this is a problem we have to solve . |
| os meus vizinhos ouviram sobre esta ideia. | my neighbors heard about this idea . | and my neighboring homes heard about this idea . |

**ObservaÃ§Ã£o:** em geral, o modelo apresenta boa fluÃªncia e cobertura; divergÃªncias ocorrem em detalhes semÃ¢nticos e escolhas lexicais.



## Pontos Positivos

### Arquitetura & Desempenho
- **ParalelizaÃ§Ã£o superior:** processa tokens em paralelo (vs. RNNs sequenciais), acelerando o treino.
- **DependÃªncias longas:** atenÃ§Ã£o captura relaÃ§Ãµes de longo alcance sem degradaÃ§Ã£o de gradiente.
- **Flexibilidade:** nÃ£o assume estrutura temporal/espacial especÃ­fica dos dados.
- **GeneralizaÃ§Ã£o:** translitera termos raros (ex.: â€œtriceratopsâ€ â†’ â€œtrigatotysâ€).

### ImplementaÃ§Ã£o
- **CÃ³digo modular e limpo**, favorecendo reutilizaÃ§Ã£o.
- **VisualizaÃ§Ãµes de atenÃ§Ã£o** para interpretabilidade.
- **ExportaÃ§Ã£o como SavedModel** para servir em produÃ§Ã£o.
- **CarÃ¡ter didÃ¡tico:** comentÃ¡rios e seÃ§Ãµes guiadas no notebook.



## Pontos Negativos

### LimitaÃ§Ãµes Computacionais
- **Custo de memÃ³ria:** atenÃ§Ã£o quadrÃ¡tica **O(nÂ²)** limita sequÃªncias longas.
- **HiperparÃ¢metros reduzidos** (d_model=128) por restriÃ§Ã£o de hardware.
- **Dataset modesto** (~50k) para qualidade de produÃ§Ã£o.

### LimitaÃ§Ãµes de Design
- **Sem transfer learning** (p.ex., mT5/T5/BART).
- **Sem beam search** (usa **greedy decoding**).
- **Positional encoding fixa** (sinusoidal) pode limitar sequÃªncias extensas.

### LimitaÃ§Ãµes PrÃ¡ticas
- **Risco de overfitting:** regularizaÃ§Ã£o alÃ©m de dropout nÃ£o explorada.
- **AvaliaÃ§Ã£o simplificada:** sem mÃ©tricas padrÃ£o como **BLEU**.
- **InferÃªncia sequencial no decoder**, limitando a velocidade.


## CPU vs GPU

### Treino em CPU
- **Tempo por Ã©poca:** ~45â€“58s (com logs desta execuÃ§Ã£o especÃ­fica).
- **Gargalos:** multiplicaÃ§Ãµes de matrizes e atenÃ§Ã£o em lote.
- **Viabilidade:** prototipagem/didÃ¡tica e datasets pequenos.

### Treino em GPU
- **Vantagens:** speedup **10â€“50Ã—** em operaÃ§Ãµes paralelizÃ¡veis.
- **Batch maior:** memÃ³ria dedicada permite `batch_size > 64`.
- **HiperparÃ¢metros realistas:** viabiliza `d_model=512`, `num_layers=6`.
- **Requisito:** GPU **â‰¥ 8 GB VRAM** recomendada.

**RecomendaÃ§Ãµes:**  
- **CPU:** aprendizado/debugging.  
- **GPU:** treino sÃ©rio.  
- **Cloud/TPUs:** Colab Pro/TPU para experimentaÃ§Ã£o rÃ¡pida.

## Estrutura do RepositÃ³rio

```
â”œâ”€â”€ README.md
â”œâ”€â”€ transformer.ipynb      # notebook principal (execuÃ§Ã£o e anÃ¡lise)
â””â”€â”€ requirements.txt       # dependÃªncias do projeto
```

---

## ğŸ“œ LicenÃ§a & CrÃ©ditos

- **Baseado em:** Tutorial oficial do TensorFlow â€œNeural machine translation with a Transformer and Kerasâ€.
- **Paper:** Vaswani et al., 2017 â€” _Attention Is All You Need_.

Este repositÃ³rio tem propÃ³sito **educacional** e de **documentaÃ§Ã£o**. Para casos de produÃ§Ã£o, recomendaâ€‘se **fineâ€‘tuning de modelos prÃ©â€‘treinados** e pipelines de avaliaÃ§Ã£o robustos.
